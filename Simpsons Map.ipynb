{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fd8c15",
   "metadata": {},
   "source": [
    "## MVP 3 - Simpson's Guest Characters Map\n",
    "This MVP scrapes the wikipedia pages for the simpsons guest stars, finds their birthplace from wikipedia, and then uses a geocoding API to find the latitude and longitude of the birthplaces. Then, the count of guest voices is mapped with donuts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import csv\n",
    "from collections import Counter\n",
    "import time\n",
    "import pandas as pd\n",
    "import folium\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea597a",
   "metadata": {},
   "source": [
    "### Get list of actors wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa463d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Wikipedia API endpoint and pages\n",
    "endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "pages = [\"List_of_The_Simpsons_guest_stars_(seasons_1–20)\", \"List_of_The_Simpsons_guest_stars_(seasons_21–present)\"]\n",
    "\n",
    "# Create a set of unique actor names\n",
    "actor_pages = set()\n",
    "for page in pages:\n",
    "    # Specify parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": page,\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"text\",\n",
    "    }\n",
    "\n",
    "    # Send the request\n",
    "    response = requests.get(endpoint, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Get main html content\n",
    "        html_content = data['parse']['text']['*']\n",
    "        # Parse the html\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find the specific table by class\n",
    "        table = soup.find(\"table\", {\"class\": \"sortable wikitable plainrowheaders\"})\n",
    "        # If there is a table, go through it\n",
    "        if table:\n",
    "            # Loop through each row \n",
    "            for row in table.find_all(\"tr\"):\n",
    "                # Extract the title from the `scope=\"row\"` column\n",
    "                header = row.find(\"th\", {\"scope\": \"row\"})\n",
    "                if header:\n",
    "                    # Extract the title text\n",
    "                    link_tag = header.find(\"a\", href=True)\n",
    "                    if link_tag:\n",
    "                        # Get the page title of the actor\n",
    "                        page = link_tag['href'][6:]\n",
    "                        page_decode = urllib.parse.unquote(page)\n",
    "                        actor_pages.add(page_decode)\n",
    "\n",
    "# Convert set to list for unique actor names\n",
    "actors = list(actor_pages)\n",
    "print(len(actors))\n",
    "for link in actors:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3dd839",
   "metadata": {},
   "source": [
    "### Get Birthplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Start a list to store birth places\n",
    "birth_places = []\n",
    "birth_places_cleaned = []\n",
    "\n",
    "\n",
    "# Define the Wikipedia API endpoint\n",
    "endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "for actor in actors:\n",
    "    try:\n",
    "        # Specify parameters for the API request\n",
    "        params = {\n",
    "            \"action\": \"parse\",\n",
    "            \"page\": f\"{actor}\", \n",
    "            \"format\": \"json\",\n",
    "            \"prop\": \"text\",\n",
    "            \"redirects\": 1\n",
    "            # To get the main HTML content\n",
    "        }\n",
    "\n",
    "        # Send the GET request\n",
    "        response = requests.get(endpoint, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            html_content = data['parse']['text']['*']  # Get the main HTML content\n",
    "\n",
    "            # Parse the HTML with Beautiful Soup\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            #Start finding birthplaces\n",
    "            # First we check the birthplace div container\n",
    "            birthplace = soup.find('div', {'class': 'birthplace'})\n",
    "            if birthplace:\n",
    "                #Get birthplace text and append to list\n",
    "                birthplace = birthplace.get_text(strip=True)\n",
    "                birth_places.append(birthplace)\n",
    "            # Otherwise we check the wikipedia link to the birthplace and export that\n",
    "            else:\n",
    "                # Find the box containing the birth information\n",
    "                infobox = soup.find('table', {'class': 'infobox'})\n",
    "                # Find the row following \"Born\" for information information\n",
    "                born_row = infobox.find('th', string=\"Born\")\n",
    "                # Extract the city link\n",
    "                if born_row:\n",
    "                    #Find the next td section after Born is mentioned\n",
    "                    birth_info_cell = born_row.find_next('td')\n",
    "                    # Find the first <a> tag for city\n",
    "                    birth_info = birth_info_cell.find('a')\n",
    "                    if birth_info:\n",
    "                        #Format and append\n",
    "                        birth_city_info = birth_info['href'][6:]\n",
    "                        birth_places.append(birth_city_info)\n",
    "                # We also need to check if the page uses Born: with the colon. \n",
    "                else:\n",
    "                    born_row = infobox.find('th', string=\"Born:\")\n",
    "                    # Extract the city link\n",
    "                    if born_row:\n",
    "                        birth_info_cell = born_row.find_next('td')\n",
    "                        # Find the first <a> tag for city\n",
    "                        birth_info = birth_info_cell.find('a')\n",
    "                        if birth_info:\n",
    "                            #Format and append\n",
    "                            birth_city_info = birth_info['href'][6:]\n",
    "                            birth_places.append(birth_city_info)\n",
    "    except:\n",
    "        print(f\"Invalid URL for {actor}\")\n",
    "                    \n",
    "print(birth_places)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf5947",
   "metadata": {},
   "source": [
    "### Clean up birth places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove periods from birth places to prevent breaking geocoding API\n",
    "cleaned_places = []\n",
    "for place in birth_places:\n",
    "    cleaned_place = place.replace(\".\",\"\")\n",
    "    cleaned_places.append(cleaned_place)\n",
    "    \n",
    "unique_places = list(set(cleaned_places))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4d683",
   "metadata": {},
   "source": [
    "### Convert Birthplaces and Count to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8be3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each city\n",
    "city_counts = Counter(cleaned_places)\n",
    "\n",
    "# Prepare data for csv\n",
    "city_data = list(city_counts.items())  # List of tuples (city_name, count)\n",
    "# Choose csv name\n",
    "csv_filename = \"birthplaces.csv\"\n",
    "\n",
    "# Write to csv\n",
    "with open(csv_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Header row\n",
    "    writer.writerow([\"City\", \"Count\"])\n",
    "    # Write each city and count in separate rows\n",
    "    writer.writerows(city_data)\n",
    "\n",
    "print(f\"Places and count have been saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d880c",
   "metadata": {},
   "source": [
    "### Read Data from CSV (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529729fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the csv\n",
    "csv_filename = \"birthplaces.csv\"\n",
    "\n",
    "# List to store city names\n",
    "cities_list = []\n",
    "\n",
    "# Read the data from the CSV\n",
    "with open(csv_filename, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    # Skip the header\n",
    "    next(reader)\n",
    "    # Read the rows\n",
    "    for row in reader:\n",
    "        city_name = row[0]\n",
    "        city_count = int(row[1])\n",
    "        # Repeat the city name as many times as the count\n",
    "        cities_list.extend([city_name] * city_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca796f",
   "metadata": {},
   "source": [
    "### Get Lat and Long from Geocoding API for each city and add to dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count places again\n",
    "place_counts = Counter(cleaned_places)\n",
    "\n",
    "# Dictionary to store place, count, lat, and lon\n",
    "place_data = {}\n",
    "\n",
    "# Loop through list to acccess geocoding API and get lat and lon based on place name\n",
    "for place in unique_places:\n",
    "    try:\n",
    "        # API allows for one request a second\n",
    "        time.sleep(1)\n",
    "        api = f'https://geocode.maps.co/search?q={place}&api_key={key}'\n",
    "        response = requests.get(api)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            location_data = data[0]\n",
    "            lat = location_data['lat']\n",
    "            lon = location_data['lon']\n",
    "            # Add to dictionary with count, lat, and lon\n",
    "            place_data[place] = {\n",
    "                \"count\": place_counts[place],\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon\n",
    "            }\n",
    "        else:\n",
    "            print(\"Bad request.\")\n",
    "    except:\n",
    "        print(f\"No location found for {place}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5d3a3",
   "metadata": {},
   "source": [
    "### Write Place, Count, Latitude, and Longitude to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63242ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('place_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Place', 'Count', 'Latitude', 'Longitude'])\n",
    "    # Write the data for each place\n",
    "    for place, data in place_data.items():\n",
    "        writer.writerow([place, data['count'], data['lat'], data['lon']])\n",
    "\n",
    "print(\"CSV file created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81634a13",
   "metadata": {},
   "source": [
    "### Create map of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv with place data\n",
    "df = pd.read_csv('place_data.csv')\n",
    "\n",
    "# Prepare the data for DBSCAN\n",
    "coords = df[['Latitude', 'Longitude']].values\n",
    "\n",
    "# Apply DBSCAN clustering to group nearby cities to eachother\n",
    "# Prevents too many small points\n",
    "db = DBSCAN(eps=0.009, min_samples=1, metric='haversine').fit(np.radians(coords))\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "df['cluster'] = db.labels_\n",
    "\n",
    "# Find the most common city (highest count) for each cluster\n",
    "most_common = df.loc[df.groupby('cluster')['Count'].idxmax()]\n",
    "\n",
    "# Group the cities by clusters and sum the counts for each cluster\n",
    "cluster_counts = df.groupby('cluster').agg({'Count': 'sum', 'Latitude': 'mean', 'Longitude': 'mean'}).reset_index()\n",
    "\n",
    "# Add the most common city to the cluster data\n",
    "cluster_counts['Place'] = most_common.set_index('cluster').loc[cluster_counts['cluster'], 'Place']\n",
    "\n",
    "# Create a base Folium map\n",
    "simpsons_map = folium.Map(location=[39.8283, -98.5795], tiles='Esri.WorldGrayCanvas', zoom_start=4)\n",
    "\n",
    "# Scaling function to prevent weird sizing\n",
    "def scale_size(count):\n",
    "    if count == 1:\n",
    "        return 10\n",
    "    elif count <= 2:\n",
    "        return count * 7.5\n",
    "    return 10 + math.log(count) * 14 \n",
    "\n",
    "# Add clusters to the map\n",
    "for _, row in cluster_counts.iterrows():\n",
    "    size = scale_size(row['Count'])\n",
    "    # Calculate icon size based on the 'Count' value.\n",
    "    # Use log to allow for variation\n",
    "    # Define a custom donut icon with dynamic size\n",
    "    icon = folium.CustomIcon(\n",
    "        icon_image='donut.png',\n",
    "        icon_size=(size, size),\n",
    "    )\n",
    "    # Add a marker with the donut icon\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        icon=icon,\n",
    "        popup=f\"{row['Count']} voice(s) from {row['Place']}\",\n",
    "    ).add_to(simpsons_map)\n",
    "\n",
    "\n",
    "# Save the map and view\n",
    "simpsons_map.save('simpsons_map.html')\n",
    "simpsons_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
